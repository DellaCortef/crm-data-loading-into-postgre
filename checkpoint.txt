1. creating python env -> venv (virtual env)

2. installing streamlit lib only into data-pipeline project

3. creating app.py file 
    - app.py is a lib that transform any Python script into web application

4. codinf: streamlit run app.py
    streamlit creates webpage running our application

5. inputing values into variables seller_email, purchase_data, purchase_time, product_value, product_quantity, and product_type

6. creating contract.py file to add data contract:
    the main objective is to ensure data gvernance when users are filling in the data

7. we are going to use pydantic lib to verify the filled information:
    pydantic lib is a lib that has series of verification coded
    lib pydantic already has email validations, e.g.
    pydantic has error messages

8. installing pydantic lib:
    cd to data-pipeline-pocket-reference
    source .venv/bin/activate
    pip install pydantic
    pip install email-validator tzdata

9. creating our classes validations:
    email
    date_time:
    product_value
    product_quantity
    product_type -> it will be a validate_call (enumerate options)

10. loading the information into Postgre
    we will use render to run 24/7 open source without credit card

11. creating database on render

12. downloading pgAdmin to connect our database

13. creating our sales table

14. creating the connection of our code to the database
    it needs to create a database.py file
    installing the psycopg2, sqlalchemy, dotenv libs

15. deploying my streamlit to people help me to fill the data

16. installing mkdocs to document our app
    pip install mkdocs mkdocs-material mkdocstrings mkdocstrings-python

17. after all, we can document the classes, functions, and so on
    doc Sales
    doc load_into_postgre
    we need to run mkdocs serve on terminal

-- changing repo name